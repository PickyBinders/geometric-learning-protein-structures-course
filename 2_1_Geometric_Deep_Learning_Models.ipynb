{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: just load a regular linear layer, a classic MLP, and a GAT model and run some graphs through it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain what needs to happen in training, validation and test steps with a diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as L\n",
    "from torch_geometric.nn import GAT\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GATModule(L.LightningModule):\n",
    "    \"\"\"\n",
    "    LightningModule wrapping a GAT model.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, num_heads, out_channels, dropout, jk):\n",
    "        super(GATModule, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = (torch.empty(32, in_channels), torch.zeros(2, 100, dtype=torch.long))\n",
    "        self.model = GAT(in_channels=in_channels,\n",
    "                         hidden_channels=hidden_channels,\n",
    "                         num_layers=num_layers,\n",
    "                         heads=num_heads,\n",
    "                         out_channels=out_channels,\n",
    "                         dropout=dropout,\n",
    "                         jk=jk, v2=True) # TODO: try other models\n",
    "\n",
    "    def forward(self, node_attributes, edge_index):\n",
    "        return self.model(node_attributes, edge_index)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self(batch.x, batch.edge_index)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, batch.y.view(-1, 1))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, sync_dist=True,\n",
    "                 batch_size=batch.x.shape[0])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(batch.x, batch.edge_index)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, batch.y.view(-1, 1))\n",
    "        self.log('hp_metric', loss, on_step=True, on_epoch=True, sync_dist=True,\n",
    "                 batch_size=batch.x.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      return torch.optim.Adam(params=self.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader import ProteinDataModule\n",
    "\n",
    "def train_model():\n",
    "  model = GATModule(\n",
    "    in_channels=20,\n",
    "    num_layers=3,\n",
    "    hidden_channels=128,\n",
    "    num_heads=4,\n",
    "    out_channels=1,\n",
    "    dropout=0.01,\n",
    "    jk='cat',\n",
    "  )\n",
    "\n",
    "  dataloader = ProteinDataModule(\n",
    "    root=\"./data\",\n",
    "    dataset_file=\"dataset.txt\",\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "  )\n",
    "\n",
    "  trainer = L.Trainer(\n",
    "      devices=\"auto\",\n",
    "      accelerator=\"auto\",\n",
    "      enable_progress_bar=True,\n",
    "      max_epochs=5,\n",
    "  )\n",
    "\n",
    "  trainer.fit(model=model, datamodule=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Other models, add custom layers, losses etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus:\n",
    "- how would you change things for protein-protein input?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
