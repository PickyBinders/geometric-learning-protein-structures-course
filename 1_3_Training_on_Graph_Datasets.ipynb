{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specificy how these Data objects within the created Dataset are split into training, validation and test sets. This is where PyTorch Lightning's DataModule comes in. The DataModule is a class that encapsulates the logic for loading, batching and splitting the data. It's a way of keeping the data loading and batching logic separate from the model and training logic, which makes the code more modular and easier to maintain. It also makes it easier to switch between different datasets and data loading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from src import dataloader\n",
    "\n",
    "class ProteinGraphDataModule(L.LightningDataModule):\n",
    "    def __init__(self, root, dataset_file, pre_transform, transform):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.dataset_file = dataset_file\n",
    "        with open(dataset_file) as f:\n",
    "            self.protein_names = [line.strip() for line in f]\n",
    "        self.protein_names = self.protein_names[:10] # SMALL DATASET FOR TESTING\n",
    "        self.pre_transform = pre_transform\n",
    "        self.transform = transform\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download, IO, etc. Useful with shared filesystems\n",
    "        # only called on 1 GPU/TPU in distributed settings\n",
    "        # does the downloading and saving of graphein graphs part, just once\n",
    "        dataloader.ProteinDataset(root=self.root,\n",
    "                                  protein_names=self.protein_names,\n",
    "                                  pre_transform=self.pre_transform, \n",
    "                                  transform=self.transform)\n",
    "    \n",
    "\n",
    "    def setup(self, stage):\n",
    "        # make assignments here (val/train/test split)\n",
    "        # called on every process in DDP\n",
    "        # now it's just loaded and not downloaded processed etc.\n",
    "        dataset = dataloader.ProteinDataset(root=self.root,\n",
    "                                  protein_names=self.protein_names,\n",
    "                                  pre_transform=self.pre_transform, \n",
    "                                  transform=self.transform)\n",
    "        train_idx, val_idx, test_idx = random_split(range(len(dataset)), [0.8, 0.1, 0.1])\n",
    "        self.train, self.val, self.test = dataset[list(train_idx)], dataset[list(val_idx)], dataset[list(test_idx)]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain dataloaders, batching etc.\n",
    "TODO: add batch_size num_workers\n",
    "TODO: how would it change if you had a predefined train/val/test split\n",
    "TODO: make a datamodule and loop through the dataloaders to show how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
